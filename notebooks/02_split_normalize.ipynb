{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook 02 — Split, Label, Metadata, Sidecars\n",
    "Goal: build corpora by competition → split into atomic Q&A → label topics/stage → extract metadata → write MD/JSONL/CSVs."
   ],
   "id": "874441894f234045"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load inputs & quick sanity\n",
    "\n",
    "Purpose: load the artifacts from Notebook 01 (page_dump.jsonl, page_sections.csv) and join them."
   ],
   "id": "77b3848a20f6b895"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T13:55:16.488739Z",
     "start_time": "2025-08-14T13:55:15.989227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 0) Setup, inputs, and sanity ---\n",
    "from pathlib import Path\n",
    "import json, pandas as pd\n",
    "\n",
    "# Resolve project root (same layout as Notebook 01)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT = NOTEBOOK_DIR.parent\n",
    "RAW_DIR        = ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR  = ROOT / \"data\" / \"processed\"\n",
    "MD_DIR         = PROCESSED_DIR / \"md\"\n",
    "JSONL_DIR      = PROCESSED_DIR / \"jsonl\"\n",
    "CSV_DIR        = PROCESSED_DIR / \"csv\"\n",
    "\n",
    "IN_PAGE_JSONL    = PROCESSED_DIR / \"page_dump.jsonl\"\n",
    "IN_PAGE_SECTIONS = PROCESSED_DIR / \"page_sections.csv\"\n",
    "\n",
    "assert IN_PAGE_JSONL.exists(), f\"Missing {IN_PAGE_JSONL}\"\n",
    "assert IN_PAGE_SECTIONS.exists(), f\"Missing {IN_PAGE_SECTIONS}\"\n",
    "\n",
    "# Load pages\n",
    "pages = [json.loads(l) for l in open(IN_PAGE_JSONL, \"r\", encoding=\"utf-8\")]\n",
    "df_sections = pd.read_csv(IN_PAGE_SECTIONS)\n",
    "\n",
    "# Attach competition to each page dict\n",
    "comp_map = dict(zip(df_sections[\"page\"], df_sections[\"competition\"]))\n",
    "for p in pages:\n",
    "    p[\"competition\"] = comp_map.get(p[\"page\"])\n",
    "\n",
    "print(\"Pages:\", len(pages))\n",
    "print(\"By competition:\\n\", df_sections[\"competition\"].value_counts())\n",
    "print(\"Preview p1 (raw):\\n\", pages[0][\"text\"][:200])"
   ],
   "id": "f3105f10e2f25723",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages: 28\n",
      "By competition:\n",
      " competition\n",
      "HSS          14\n",
      "ADRES        10\n",
      "E-TICARET     4\n",
      "Name: count, dtype: int64\n",
      "Preview p1 (raw):\n",
      " Hava Savunma S)stemler) Yarışması \n",
      "1. Hava Savunma S+stemler+ Yarışması'nın temel amacı ned+r? \n",
      "Yarışmanın amacı, takımların ver+len senaryolara uygun görevler+ başarıyla yer+ne get+recek \n",
      "hava savunm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Normalize again & build corpora with page spans\n",
    "\n",
    "Purpose: re-use a light normalizer, then create a single text per competition plus page offset spans (so later we can map Q&A chunks to page ranges)."
   ],
   "id": "6d084b31a3eee994"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:20:06.090446Z",
     "start_time": "2025-08-14T14:20:06.075202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1) Normalize text & build corpora (text + page_spans) ---\n",
    "import re\n",
    "\n",
    "SOFT_HYPHEN = \"\\u00ad\"\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.replace(SOFT_HYPHEN, \"\").replace(\"\\t\", \" \")\n",
    "    s = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    s = re.sub(r\"(?m)^(\\s*\\d+)\\.(\\S)\", r\"\\1. \\2\", s)\n",
    "    return s\n",
    "\n",
    "for p in pages:\n",
    "    p[\"text_norm\"] = normalize_text(p[\"text\"])\n",
    "\n",
    "# Build corpora ordered by page\n",
    "corpora = {}\n",
    "for p in pages:\n",
    "    comp = p[\"competition\"]\n",
    "    if comp is None:\n",
    "        continue\n",
    "    if comp not in corpora:\n",
    "        corpora[comp] = {\"text\": \"\", \"page_spans\": []}\n",
    "    start = len(corpora[comp][\"text\"])\n",
    "    corpora[comp][\"text\"] += p[\"text_norm\"] + \"\\n\\n\"\n",
    "    end = len(corpora[comp][\"text\"])\n",
    "    corpora[comp][\"page_spans\"].append({\"page\": p[\"page\"], \"start\": start, \"end\": end})\n",
    "\n",
    "# Quick stats\n",
    "for comp, bundle in corpora.items():\n",
    "    print(f\"{comp}: chars={len(bundle['text'])} | pages={len(bundle['page_spans'])}\")\n",
    "    # tiny preview\n",
    "    print(\"  preview:\", bundle[\"text\"][:120].replace(\"\\n\",\" \") + \"…\")"
   ],
   "id": "ae912e8c33aca3f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSS: chars=30030 | pages=14\n",
      "  preview: Hava Savunma S)stemler) Yarışması  1. Hava Savunma S+stemler+ Yarışması'nın temel amacı ned+r?  Yarışmanın amacı, takıml…\n",
      "E-TICARET: chars=10875 | pages=4\n",
      "  preview: teslimlerine, itiraz süreçlerinden üye ekleme/çıkarma işlemlerine ve resmi duyurulara kadar  tüm organizasyonel faaliyet…\n",
      "ADRES: chars=24217 | pages=10\n",
      "  preview: sıralama yapmaları beklenir. Ayrıca, bu aşama \"Önyüz Tasarlama\" görevini de içerdiği için26,  takımların   React, Vue gi…\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q&A patterns + splitters (with robust fallbacks)\n",
    "Purpose: split into one question + its answer per item. Priorities:\n",
    "\t1.\tNumbered questions (HSS/ADRES),\n",
    "\t2.\t“Soru:” fallback,\n",
    "\t3.\tGeneric lines ending with “?” (covers E-Ticaret prose).\n",
    "\n",
    "We also map each QA back to page_start–page_end"
   ],
   "id": "5442ee051475a641"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:20:09.498438Z",
     "start_time": "2025-08-14T14:20:09.481453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2) Q&A splitting (one question + its answer) ---\n",
    "import re\n",
    "\n",
    "# 1) Numbered questions (the \"?\" is optional in this doc)\n",
    "Q_PATTERN = re.compile(r\"(?m)^\\s*(\\d+)\\.\\s+(.{1,200}?)(\\?)?\\s*$\")\n",
    "# 2) \"Soru:\" fallback\n",
    "Q_FALLBACK = re.compile(r\"(?m)^\\s*Soru[:：]\\s+(.{1,200}?)(\\?)?\\s*$\", re.IGNORECASE)\n",
    "# 3) Generic question line at **paragraph start** (covers prose, avoids in-answer rhetoricals)\n",
    "Q_GENERIC = re.compile(r\"(?m)(?:^|\\n\\n)(?P<q>.{4,200}\\?)\\s*$\")\n",
    "\n",
    "# Apply generic-question fallback only to prose-style sections\n",
    "GENERIC_ALLOWED = {\"E-TICARET\", \"ADRES\"}\n",
    "\n",
    "MIN_ANS_CHARS = 20  # skip headers/noise\n",
    "\n",
    "def _norm_qtext(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    s = s[:-1] if s.endswith(\"?\") else s\n",
    "    return s\n",
    "\n",
    "def page_range_from_offsets(comp: str, start_idx: int, end_idx: int):\n",
    "    spans = corpora[comp][\"page_spans\"]\n",
    "    pages_hit = [s[\"page\"] for s in spans if not (end_idx <= s[\"start\"] or start_idx >= s[\"end\"])]\n",
    "    if not pages_hit:\n",
    "        return None, None\n",
    "    return min(pages_hit), max(pages_hit)\n",
    "\n",
    "def split_by_matches(text: str, comp: str, matches, with_numbers=True, tag=\"Q\"):\n",
    "    qas, matches = [], list(matches)\n",
    "    for i, m in enumerate(matches):\n",
    "        if with_numbers:\n",
    "            q_num = int(m.group(1))\n",
    "            question = m.group(2).strip()\n",
    "            qa_id = f\"{comp}-{tag}{q_num:03d}\"\n",
    "            q_start = m.start()\n",
    "        else:\n",
    "            q_num = i + 1\n",
    "            # \"Soru:\" fallback (group 1) or generic (?P<q>)\n",
    "            question = (m.group(1) if m.groups() else m.group(\"q\")).strip()\n",
    "            qa_id = f\"{comp}-{tag}{q_num:03d}\"\n",
    "            q_start = m.start()\n",
    "\n",
    "        ans_start = m.end()\n",
    "        ans_end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        answer = text[ans_start:ans_end].strip()\n",
    "\n",
    "        if len(answer) < MIN_ANS_CHARS:\n",
    "            continue\n",
    "\n",
    "        page_start, page_end = page_range_from_offsets(comp, q_start, ans_end)\n",
    "        qas.append({\n",
    "            \"qa_id\": qa_id,\n",
    "            \"competition\": comp,\n",
    "            \"page_start\": page_start, \"page_end\": page_end,\n",
    "            \"question\": question, \"answer\": answer,\n",
    "        })\n",
    "    return qas\n",
    "\n",
    "def split_qa_for_comp(comp: str):\n",
    "    text = corpora[comp][\"text\"]\n",
    "\n",
    "    # Collect candidates from all matchers\n",
    "    qas_all = []\n",
    "\n",
    "    # Priority tags help you later, but we UNION them here\n",
    "    qas_all += split_by_matches(text, comp, Q_PATTERN.finditer(text), with_numbers=True,  tag=\"Q\")\n",
    "    qas_all += split_by_matches(text, comp, Q_FALLBACK.finditer(text), with_numbers=False, tag=\"QF\")\n",
    "    # Only apply generic-question matcher to prose sections (avoid double-count in HSS)\n",
    "    if comp in GENERIC_ALLOWED:\n",
    "        qas_all += split_by_matches(text, comp, Q_GENERIC.finditer(text), with_numbers=False, tag=\"QG\")\n",
    "\n",
    "    # Deduplicate: same competition + page_start + normalized question\n",
    "    seen, deduped = set(), []\n",
    "    for qa in sorted(qas_all, key=lambda r: (r[\"page_start\"] or 0, r[\"qa_id\"])):\n",
    "        key = (qa[\"competition\"], qa[\"page_start\"], _norm_qtext(qa[\"question\"]))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        deduped.append(qa)\n",
    "\n",
    "    return deduped\n",
    "\n",
    "all_qas = []\n",
    "for comp in corpora:\n",
    "    all_qas.extend(split_qa_for_comp(comp))"
   ],
   "id": "db4a0f131387780f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Quick checks (totals + per-competition) & tiny preview\n",
    "Purpose: verify we captured all three sections (E-Ticaret should be > 0 now) and see a sample."
   ],
   "id": "81610f3b571683d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:20:13.874939Z",
     "start_time": "2025-08-14T14:20:13.855698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3) Q&A split checks ---\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_qas)\n",
    "print(\"QAs total:\", len(df))\n",
    "print(\"QAs per competition:\\n\", df[\"competition\"].value_counts(dropna=False))\n",
    "\n",
    "# show 1 example per competition\n",
    "for comp in df[\"competition\"].unique():\n",
    "    row = df[df[\"competition\"]==comp].iloc[0]\n",
    "    print(f\"\\n[{comp}] {row['qa_id']}\\nQ:\", row[\"question\"][:120], \"\\nA:\", row[\"answer\"][:200], \"…\")"
   ],
   "id": "4084a9f0d251d7a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAs total: 102\n",
      "QAs per competition:\n",
      " competition\n",
      "HSS          51\n",
      "ADRES        45\n",
      "E-TICARET     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[HSS] HSS-Q001\n",
      "Q: Hava Savunma S+stemler+ Yarışması'nın temel amacı ned+r \n",
      "A: Yarışmanın amacı, takımların ver+len senaryolara uygun görevler+ başarıyla yer+ne get+recek \n",
      "hava savunma s+stemler+ gel+şt+rmes+ ve üretmes+d+r. Aynı zamanda, hava savunma \n",
      "s+stemler+n+n önem+n+n ülk …\n",
      "\n",
      "[E-TICARET] E-TICARET-QG001\n",
      "Q: önemi nedir? Bu süre takımlara ne gibi avantajlar sağlar? \n",
      "A: E-Ticaret Hackathonu takviminde, ﬁnalistlerin açıklanması ile ﬁziksel etkinliğin başlaması \n",
      "arasında yaklaşık 15 günlük bir süre bırakılması66, takımlara stratejik bir hazırlık ve proje \n",
      "olgunlaştırma …\n",
      "\n",
      "[ADRES] ADRES-QG001\n",
      "Q: Yarışmanın temel amacı nedir? \n",
      "A: Yarışmanın temel amacı, özellikle son kilometre teslimatlarında yaşanan adres verisi \n",
      "tutarsızlıkları ve yönetim sorunlarına yönelik yapay zeka, doğal dil işleme (NLP), coğraﬁ bilgi \n",
      "sistemleri (GIS)  …\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Topic labeling (regex, first-match wins)\n",
    "\n",
    "Purpose: assign topic for routing/filters (eligibility, team, stages, scoring, penalties, logistics, timeline, other)."
   ],
   "id": "7744e61e2d6582f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:31:30.623645Z",
     "start_time": "2025-08-14T14:31:30.570686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4) Topic labeling ---\n",
    "import re\n",
    "\n",
    "TOPIC_RULES = {\n",
    "    \"eligibility\": r\"(katılım|kimler|başvuru koşul|uygun|gereklilik|şart)\",\n",
    "    \"team\":        r\"(takım|üye|danışman|rol|katılımcı sayısı)\",\n",
    "    \"stages\":      r\"(aşama|görev|süreç|workflow|sunum)\",\n",
    "    \"scoring\":     r\"(puan|puanlama|bsp|bonus|değerlendir|kriter|puan kesinti)\",\n",
    "    \"penalties\":   r\"(ceza\\w*|diskalifiye\\w*|ihlal\\w*|yasak\\w*|kural\\s*dışı|kuraldışı|dost\\s+(hedef|ateş[iı]))\",\n",
    "    \"logistics\":   r\"(konaklama|ulaşım|destek|sponsor|mekan|yer|konaklama)\",\n",
    "    \"timeline\":    r\"(son başvuru|son tarih|takvim|deadline|program|başvuru tarihi|teslim tarihi)\",\n",
    "}\n",
    "TOPIC_RE = {k: re.compile(v, re.IGNORECASE) for k,v in TOPIC_RULES.items()}\n",
    "\n",
    "def label_topic(q, a):\n",
    "    text = f\"{q}\\n{a}\"\n",
    "    for name, rx in TOPIC_RE.items():\n",
    "        if rx.search(text):\n",
    "            return name\n",
    "    return \"other\"\n",
    "\n",
    "df[\"topic\"] = [label_topic(q, a) for q,a in zip(df[\"question\"], df[\"answer\"])]\n",
    "print(\"Topic counts:\\n\", df[\"topic\"].value_counts())"
   ],
   "id": "bdaed3256d143313",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic counts:\n",
      " topic\n",
      "team           49\n",
      "eligibility    33\n",
      "stages          9\n",
      "other           6\n",
      "logistics       2\n",
      "scoring         2\n",
      "timeline        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stage detection (Aşama 1/2/3 and roman numerals)\n",
    "\n",
    "Purpose: fill the stage column when “aşama”/“stage” is mentioned near a number."
   ],
   "id": "c5cbf1bb8eac626e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:40:33.508114Z",
     "start_time": "2025-08-14T14:40:33.484110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 5) Stage detection ---\n",
    "import re\n",
    "ROMAN = {\"i\":1, \"ii\":2, \"iii\":3, \"iv\":4, \"v\":5}\n",
    "\n",
    "def detect_stage(text: str):\n",
    "    t = text.lower()\n",
    "    m = re.search(r\"(?:aşama|stage)\\s*[:\\-]?\\s*(\\d+)\", t)\n",
    "    if m: return int(m.group(1))\n",
    "    m = re.search(r\"(?:aşama|stage)\\s*[:\\-]?\\s*([ivx]{1,3})\\b\", t)\n",
    "    if m: return ROMAN.get(m.group(1), None)\n",
    "    m = re.search(r\"\\b([ivx]{1,3})\\.\\s*aşama\\b\", t)\n",
    "    if m: return ROMAN.get(m.group(1), None)\n",
    "    return None\n",
    "\n",
    "df[\"stage\"] = [detect_stage(f\"{q}\\n{a}\") for q,a in zip(df[\"question\"], df[\"answer\"])]\n",
    "print(\"Stage distribution:\\n\", df[\"stage\"].value_counts(dropna=False).head(10))"
   ],
   "id": "29a67951d15fe1ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage distribution:\n",
      " stage\n",
      "NaN    99\n",
      "1.0     3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stage from page headings (page context + neighbor fallback)\n",
    "\n",
    "Purpose: infer stage from page headings like “I. AŞAMA”, “Aşama 2”, etc., and fill NaNs."
   ],
   "id": "a4424759d3cdd1b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:40:34.996773Z",
     "start_time": "2025-08-14T14:40:34.981370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 5b) Stage from page context (headings + neighbor-page fallback) ---\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Reuse ROMAN from previous cell\n",
    "# ROMAN = {\"i\":1, \"ii\":2, \"iii\":3, \"iv\":4, \"v\":5}\n",
    "\n",
    "def page_stage(text: str):\n",
    "    t = text.lower()\n",
    "    # Prefer Roman headings like \"I. AŞAMA\" or \"AŞAMA II\"\n",
    "    m = re.search(r\"\\b([ivx]{1,3})\\.\\s*aşama\\b\", t)\n",
    "    if m:\n",
    "        return ROMAN.get(m.group(1))\n",
    "    m = re.search(r\"\\başama\\s*[:\\-]?\\s*([ivx]{1,3})\\b\", t)\n",
    "    if m:\n",
    "        return ROMAN.get(m.group(1))\n",
    "    # Numeric forms: \"Aşama 1\", \"Stage 2\"\n",
    "    m = re.search(r\"(?:aşama|stage)\\s*[:\\-]?\\s*(\\d+)\\b\", t)\n",
    "    if m:\n",
    "        try:\n",
    "            return int(m.group(1))\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Build a per-page stage map from normalized page text\n",
    "stage_by_page = {p[\"page\"]: page_stage(p.get(\"text_norm\") or p[\"text\"]) for p in pages}\n",
    "\n",
    "# Fill from the QA's start page when stage is NaN\n",
    "df[\"stage_page\"] = df[\"page_start\"].map(stage_by_page)\n",
    "df[\"stage\"] = df[\"stage\"].fillna(df[\"stage_page\"])\n",
    "\n",
    "# Neighbor-page fallback (within same competition)\n",
    "def neighbor_stage(row):\n",
    "    if pd.notna(row[\"stage\"]):\n",
    "        return row[\"stage\"]\n",
    "    start = row[\"page_start\"]\n",
    "    comp = row[\"competition\"]\n",
    "    for off in (-1, 1):\n",
    "        pg = (start or 0) + off\n",
    "        if pg in stage_by_page and stage_by_page[pg] is not None and comp_map.get(pg) == comp:\n",
    "            return stage_by_page[pg]\n",
    "    return None\n",
    "\n",
    "df[\"stage\"] = df.apply(lambda r: r[\"stage\"] if pd.notna(r[\"stage\"]) else neighbor_stage(r), axis=1)\n",
    "\n",
    "print(\"Stage distribution (after page-context fill):\")\n",
    "print(df[\"stage\"].value_counts(dropna=False).head(10))"
   ],
   "id": "9dfdf77c7d2718be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage distribution (after page-context fill):\n",
      "stage\n",
      "NaN    77\n",
      "1.0    25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## Metadata extraction (add fields: dates, numbers, formulas, links)",
   "id": "9bb6550481737f40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:48:54.750260Z",
     "start_time": "2025-08-14T17:48:54.673164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 6) Metadata extraction ---\n",
    "import re\n",
    "\n",
    "TR_MONTHS = {\n",
    "    \"ocak\":\"01\",\"şubat\":\"02\",\"subat\":\"02\",\"mart\":\"03\",\"nisan\":\"04\",\"mayıs\":\"05\",\"mayis\":\"05\",\n",
    "    \"haziran\":\"06\",\"temmuz\":\"07\",\"ağustos\":\"08\",\"agustos\":\"08\",\"eylül\":\"09\",\"eylul\":\"09\",\n",
    "    \"ekim\":\"10\",\"kasım\":\"11\",\"kasim\":\"11\",\"aralık\":\"12\",\"aralik\":\"12\"\n",
    "}\n",
    "\n",
    "date_dd_mon_yyyy = re.compile(r\"\\b(\\d{1,2})\\s+([a-zçğıöşü]+)\\s+(\\d{4})\\b\", re.IGNORECASE)\n",
    "date_dd_mm_yyyy  = re.compile(r\"\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{4})\\b\")\n",
    "http_re     = re.compile(r\"https?://\\S+\")\n",
    "money_re    = re.compile(r\"(?:₺|TL)\\s*\\d[\\d.,]*\", re.IGNORECASE)\n",
    "percent_re  = re.compile(r\"(?:\\b\\d{1,3}\\s*%|%\\s*\\d{1,3}\\b)\")\n",
    "points_re   = re.compile(r\"\\b\\d+\\s*puan\\b\", re.IGNORECASE)\n",
    "time_re     = re.compile(r\"\\b(\\d{1,2})\\s*(?:dk|dakika|saat|hour|min)\\b\", re.IGNORECASE)\n",
    "formula_re  = re.compile(r\"\\bBSP\\b|=|formül\", re.IGNORECASE)\n",
    "\n",
    "def extract_dates_tr(text: str):\n",
    "    dates = []\n",
    "    for d,m,y in date_dd_mon_yyyy.findall(text):\n",
    "        mm = TR_MONTHS.get(m.lower())\n",
    "        dates.append({\"raw\": f\"{d} {m} {y}\", \"iso\": f\"{y}-{mm}-{int(d):02d}\" if mm else None})\n",
    "    for d,m,y in date_dd_mm_yyyy.findall(text):\n",
    "        dates.append({\"raw\": f\"{d}.{m}.{y}\", \"iso\": f\"{y}-{int(m):02d}-{int(d):02d}\"})\n",
    "    # dedupe by raw\n",
    "    out, seen = [], set()\n",
    "    for r in dates:\n",
    "        if r[\"raw\"] in seen: continue\n",
    "        seen.add(r[\"raw\"]); out.append(r)\n",
    "    return out\n",
    "\n",
    "def extract_numbers(text: str):\n",
    "    vals = []\n",
    "    vals += percent_re.findall(text)\n",
    "    vals += points_re.findall(text)\n",
    "    vals += money_re.findall(text)\n",
    "    vals += time_re.findall(text)\n",
    "    # normalize tuples from regex groups\n",
    "    return list(dict.fromkeys([v if isinstance(v, str) else \" \".join(v) for v in vals]))\n",
    "\n",
    "def extract_formulas(text: str):\n",
    "    lines = [ln.strip() for ln in text.splitlines() if formula_re.search(ln)]\n",
    "    return lines[:5]\n",
    "\n",
    "def extract_links(text: str):\n",
    "    return http_re.findall(text)[:5]\n",
    "\n",
    "df[\"dates\"]    = [extract_dates_tr(f\"{q}\\n{a}\") for q,a in zip(df[\"question\"], df[\"answer\"])]\n",
    "df[\"numbers\"]  = [extract_numbers(f\"{q}\\n{a}\") for q,a in zip(df[\"question\"], df[\"answer\"])]\n",
    "df[\"formulas\"] = [extract_formulas(f\"{q}\\n{a}\") for q,a in zip(df[\"question\"], df[\"answer\"])]\n",
    "df[\"links\"]    = [extract_links(f\"{q}\\n{a}\") for q,a in zip(df[\"question\"], df[\"answer\"])]\n",
    "\n",
    "print(\"Has dates   :\", sum(bool(x) for x in df['dates']))\n",
    "print(\"Has numbers :\", sum(bool(x) for x in df['numbers']))\n",
    "print(\"Has formulas:\", sum(bool(x) for x in df['formulas']))\n",
    "print(\"Has links   :\", sum(bool(x) for x in df['links']))"
   ],
   "id": "81320752dccdfd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has dates   : 6\n",
      "Has numbers : 20\n",
      "Has formulas: 4\n",
      "Has links   : 1\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Write Markdown per competition (human-readable) — brief\n",
    "Purpose: save quick review files you can skim; gets overwritten on re-runs."
   ],
   "id": "3f39ae0ee18076cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:01:01.480825Z",
     "start_time": "2025-08-14T18:01:01.429461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 7) Markdown per competition ---\n",
    "MD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def md_block(row):\n",
    "    head = f\"## {row['qa_id']}: {row['question']}\".strip()\n",
    "    meta = f\"[page: {row['page_start']}-{row['page_end']}] [topic: {row['topic']}]\"\n",
    "    return f\"{head}\\n{row['answer']}\\n\\n{meta}\\n---\\n\"\n",
    "\n",
    "for comp in sorted(df[\"competition\"].unique()):\n",
    "    text = \"\".join(md_block(r) for _, r in df[df[\"competition\"]==comp].iterrows())\n",
    "    outp = MD_DIR / f\"{comp}.md\"\n",
    "    outp.write_text(text, encoding=\"utf-8\")\n",
    "    print(\"Wrote:\", outp, \"| items:\", (df[\"competition\"]==comp).sum())"
   ],
   "id": "359d0f32e2bd2a89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/macbook/T3/rag-tekno/data/processed/md/ADRES.md | items: 45\n",
      "Wrote: /Users/macbook/T3/rag-tekno/data/processed/md/E-TICARET.md | items: 6\n",
      "Wrote: /Users/macbook/T3/rag-tekno/data/processed/md/HSS.md | items: 51\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Write JSONL sidecar + optional CSVs (deadlines, scoring)\n",
    "Purpose: machine-readable data for your RAG + two small lookup tables."
   ],
   "id": "9eca23f8c6665b9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:03:08.771420Z",
     "start_time": "2025-08-14T18:03:08.685045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 8) JSONL sidecar + optional CSVs ---\n",
    "import json\n",
    "\n",
    "JSONL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "sidecar_path = JSONL_DIR / \"qa_meta.jsonl\"\n",
    "\n",
    "fields = [\"qa_id\",\"competition\",\"topic\",\"stage\",\"page_start\",\"page_end\",\n",
    "          \"question\",\"answer\",\"dates\",\"numbers\",\"formulas\",\"links\"]\n",
    "\n",
    "with open(sidecar_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, r in df[fields].iterrows():\n",
    "        f.write(json.dumps({k: r[k] for k in fields}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote:\", sidecar_path)\n",
    "\n",
    "# Optional: deadlines.csv & scoring.csv\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Deadlines: any QA with a date\n",
    "dead_rows = []\n",
    "for _, r in df.iterrows():\n",
    "    for d in r[\"dates\"]:\n",
    "        dead_rows.append({\n",
    "            \"qa_id\": r[\"qa_id\"], \"competition\": r[\"competition\"],\n",
    "            \"topic\": r[\"topic\"], \"raw_date\": d[\"raw\"], \"iso\": d[\"iso\"],\n",
    "            \"page_start\": r[\"page_start\"], \"page_end\": r[\"page_end\"],\n",
    "        })\n",
    "import pandas as pd\n",
    "pd.DataFrame(dead_rows).to_csv(CSV_DIR / \"deadlines.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", CSV_DIR / \"deadlines.csv\", \"| rows:\", len(dead_rows))\n",
    "\n",
    "# Scoring: topic in {scoring, penalties} or formulas present\n",
    "sc_mask = (df[\"topic\"].isin([\"scoring\",\"penalties\"])) | (df[\"formulas\"].astype(bool))\n",
    "sc_df = df.loc[sc_mask, [\"qa_id\",\"competition\",\"topic\",\"numbers\",\"formulas\",\"page_start\",\"page_end\"]]\n",
    "sc_df.to_csv(CSV_DIR / \"scoring.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", CSV_DIR / \"scoring.csv\", \"| rows:\", len(sc_df))"
   ],
   "id": "689ae07b884613a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/macbook/T3/rag-tekno/data/processed/jsonl/qa_meta.jsonl\n",
      "Wrote: /Users/macbook/T3/rag-tekno/data/processed/csv/deadlines.csv | rows: 9\n",
      "Wrote: /Users/macbook/T3/rag-tekno/data/processed/csv/scoring.csv | rows: 6\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Quality gates (must pass before embeddings)\n",
    "\n",
    "Purpose: sanity-check the dataset so we don’t carry bad artifacts into indexing."
   ],
   "id": "164a0bd2991b117f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:05:36.037753Z",
     "start_time": "2025-08-14T18:05:35.973377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 9) Quality gates ---\n",
    "import pandas as pd\n",
    "\n",
    "# 1) basic shape & empties\n",
    "assert len(df) > 0, \"No QAs!\"\n",
    "assert df[\"question\"].str.len().min() > 0, \"Empty question found\"\n",
    "assert df[\"answer\"].str.len().min() > 0, \"Empty answer found\"\n",
    "\n",
    "# 2) competitions & per-comp coverage\n",
    "allowed = {\"HSS\",\"E-TICARET\",\"ADRES\"}\n",
    "assert set(df[\"competition\"].unique()) <= allowed, f\"Unexpected competition labels: {set(df['competition'].unique())-allowed}\"\n",
    "per_comp = df[\"competition\"].value_counts()\n",
    "assert (per_comp > 0).all(), \"Some competition has zero items\"\n",
    "print(\"Per-competition counts:\\n\", per_comp.to_string())\n",
    "\n",
    "# 3) duplicates (same competition + question)\n",
    "dup_mask = df.duplicated(subset=[\"competition\",\"question\"], keep=False)\n",
    "print(\"Duplicates by (competition, question):\", int(dup_mask.sum()))\n",
    "if dup_mask.any():\n",
    "    print(df.loc[dup_mask, [\"competition\",\"qa_id\",\"question\"]].head(5).to_string(index=False))\n",
    "\n",
    "# 4) very long answers (inspect if any)\n",
    "long_mask = df[\"answer\"].str.len() > 2000\n",
    "print(\"Very long answers (>2000 chars):\", int(long_mask.sum()))\n",
    "if long_mask.any():\n",
    "    print(df.loc[long_mask, [\"qa_id\",\"competition\",\"page_start\",\"page_end\"]].head().to_string(index=False))\n",
    "\n",
    "print(\"\\nNotebook 02 ✅ complete\")"
   ],
   "id": "ec5863f2d47bf3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-competition counts:\n",
      " competition\n",
      "HSS          51\n",
      "ADRES        45\n",
      "E-TICARET     6\n",
      "Duplicates by (competition, question): 0\n",
      "Very long answers (>2000 chars): 2\n",
      "     qa_id competition  page_start  page_end\n",
      "ADRES-Q002       ADRES          26        28\n",
      "  HSS-Q003         HSS           9        14\n",
      "\n",
      "Notebook 02 ✅ complete\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "af35bc64664f31d4"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
