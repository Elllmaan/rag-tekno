{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Goal: Normalize Turkish text, split into atomic Q&A chunks per competition, and emit Markdown, JSONL, and optional CSVs.",
   "id": "874441894f234045"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## One-time installs (if you haven’t already)",
   "id": "66bc570930c6f5d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#%pip install regex dateparser python-dateutil rapidfuzz",
   "id": "55a3f005ea899379"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load page data and section map",
   "id": "48817458d8989e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:10:46.105987Z",
     "start_time": "2025-08-14T11:10:45.491928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 0) Imports & load raw artifacts ---\n",
    "from pathlib import Path\n",
    "import re, json, math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateparser.search import search_dates\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "IN_JSONL = ROOT / \"data/processed/page_dump.jsonl\"\n",
    "PAGE_MAP_CSV = ROOT / \"data/processed/page_sections.csv\"\n",
    "OUT_MD = ROOT / \"data/processed/md\"\n",
    "OUT_JSONL = ROOT / \"data/processed/jsonl\" / \"qa_meta.jsonl\"\n",
    "OUT_DEADLINES = ROOT / \"data/processed/csv\" / \"deadlines.csv\"\n",
    "OUT_SCORING = ROOT / \"data/processed/csv\" / \"scoring.csv\"\n",
    "\n",
    "# Load\n",
    "pages = [json.loads(l) for l in open(IN_JSONL, \"r\", encoding=\"utf-8\")]\n",
    "page_map = pd.read_csv(PAGE_MAP_CSV)\n",
    "comp_by_page = dict(zip(page_map.page, page_map.competition))\n"
   ],
   "id": "254f652f12ddb3a4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Normalize Turkish text**",
   "id": "2d97edf98ffa8786"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Do a deterministic, idempotent pipeline (functions you can re-run safely):\n",
    "\n",
    "- **Unicode normalize** (NFC) and **fix common OCR artifacts**:\n",
    "\n",
    "    - Remove soft hyphens `\\u00AD`, stray `+`, duplicated spaces.\n",
    "\n",
    "    - Dehyphenate split words at linebreaks: `r\"(\\w+)-\\n(\\w+)\" → r\"\\1\\2\"`.\n",
    "\n",
    "- **Line hygiene**:\n",
    "\n",
    "    - Replace multi-newlines with a single blank line.\n",
    "\n",
    "    - Standardize bullet/number formats (e.g., `1.` → `1.` ).\n",
    "\n",
    "- **Diacritics**: If the source lost Turkish diacritics, you _can’t reliably infer them_ programmatically; keep original if present. Ensure you **don’t strip** valid diacritics (`çğıöşüÇĞİÖŞÜ`).\n",
    "\n",
    "- **Keep page anchors** so you can back-cite later."
   ],
   "id": "cedca17fea5f2fb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:11:44.489073Z",
     "start_time": "2025-08-14T11:11:44.468735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1) Normalization helpers (idempotent) ---\n",
    "SOFT_HYPHEN = \"\\u00ad\"\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # remove soft hyphens / stray plus, collapse spaces\n",
    "    s = s.replace(SOFT_HYPHEN, \"\")\n",
    "    s = s.replace(\"\\t\", \" \")\n",
    "    # dehyphenate across line breaks: \"kelime-\\nler\" -> \"kelimeler\"\n",
    "    s = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", s)\n",
    "    # unify newlines\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # remove lines that are just '+'\n",
    "    s = re.sub(r\"^\\s*\\+\\s*$\", \"\", s, flags=re.MULTILINE)\n",
    "    # collapse >2 newlines to 2\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    # ensure numbered lists have a space after \"1.\" etc\n",
    "    s = re.sub(r\"(?m)^(\\s*\\d+)\\.(\\S)\", r\"\\1. \\2\", s)\n",
    "    return s\n",
    "\n",
    "for p in pages:\n",
    "    p[\"text_norm\"] = normalize_text(p[\"text\"])\n"
   ],
   "id": "6c398dd7221382ca",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:12:39.032321Z",
     "start_time": "2025-08-14T11:12:39.024287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2) Build competition-specific corpora with page boundaries ---\n",
    "from collections import defaultdict\n",
    "\n",
    "corpora = defaultdict(lambda: {\"text\": \"\", \"page_spans\": []})\n",
    "for p in pages:\n",
    "    comp = comp_by_page.get(p[\"page\"])\n",
    "    if comp is None:\n",
    "        continue  # skip unknown pages\n",
    "    start = len(corpora[comp][\"text\"])\n",
    "    corpora[comp][\"text\"] += p[\"text_norm\"] + \"\\n\\n\"\n",
    "    end = len(corpora[comp][\"text\"])\n",
    "    corpora[comp][\"page_spans\"].append({\"page\": p[\"page\"], \"start\": start, \"end\": end})\n",
    "\n",
    "list(corpora.keys())"
   ],
   "id": "fcf7d0f7b419f635",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HSS', 'E-TICARET', 'ADRES']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Split into atomic Q&A items**",
   "id": "9f77783c63f4af1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use layered heuristics—stop at the first that yields stable splits:\n",
    "\n",
    "1. **Numbered questions** pattern (most common):\n",
    "\n",
    "    - Split on lines that start with `^\\s*\\d+\\.\\s+` and where the line **ends with `?`** within 200 chars.\n",
    "\n",
    "    - The block until the next question number is the **answer**.\n",
    "\n",
    "2. **Fallback** if the doc mixes formats:\n",
    "\n",
    "    - Consider “Soru:” / “S:” / “?” line endings as question starts (but avoid splitting inside answers by requiring a blank line before).\n",
    "\n",
    "3. **Guardrails**:\n",
    "\n",
    "    - Minimum answer length (e.g., ≥ 40 chars) to avoid headers being misread as Qs.\n",
    "\n",
    "    - Merge tiny fragments back to previous answer if a false positive occurs."
   ],
   "id": "f3f2e65538040487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:13:31.289337Z",
     "start_time": "2025-08-14T11:13:31.272383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3) Q&A splitting (one question + its answer as an atomic chunk) ---\n",
    "# Heuristic: questions are numbered lines ending with '?'\n",
    "Q_PATTERN = re.compile(r\"(?m)^\\s*(\\d+)\\.\\s+(.{1,200}?\\?)\\s*$\")\n",
    "\n",
    "def page_range_from_offsets(comp: str, start_idx: int, end_idx: int):\n",
    "    \"\"\"Map a text slice [start_idx, end_idx) to page_start..page_end using precomputed spans.\"\"\"\n",
    "    spans = corpora[comp][\"page_spans\"]\n",
    "    pages_hit = [s[\"page\"] for s in spans if not (end_idx <= s[\"start\"] or start_idx >= s[\"end\"])]\n",
    "    if not pages_hit:\n",
    "        return None, None\n",
    "    return min(pages_hit), max(pages_hit)\n",
    "\n",
    "def split_qa_for_comp(comp: str):\n",
    "    text = corpora[comp][\"text\"]\n",
    "    qas = []\n",
    "    matches = list(Q_PATTERN.finditer(text))\n",
    "    for i, m in enumerate(matches):\n",
    "        q_num = int(m.group(1))\n",
    "        question = m.group(2).strip()\n",
    "        ans_start = m.end()\n",
    "        ans_end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        answer = text[ans_start:ans_end].strip()\n",
    "\n",
    "        # guardrails: skip obviously bad splits\n",
    "        if len(answer) < 40:   # too short = likely header/noise\n",
    "            continue\n",
    "\n",
    "        page_start, page_end = page_range_from_offsets(comp, m.start(), ans_end)\n",
    "        qa = {\n",
    "            \"qa_id\": f\"{comp}-Q{q_num:03d}\",\n",
    "            \"competition\": comp,\n",
    "            \"page_start\": page_start, \"page_end\": page_end,\n",
    "            \"question\": question, \"answer\": answer,\n",
    "        }\n",
    "        qas.append(qa)\n",
    "    return qas\n",
    "\n",
    "all_qas = []\n",
    "for comp in corpora:\n",
    "    all_qas.extend(split_qa_for_comp(comp))\n",
    "\n",
    "len(all_qas), all_qas[:3]\n"
   ],
   "id": "191bc7e1653f3c5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49,\n",
       " [{'qa_id': 'HSS-Q001',\n",
       "   'competition': 'HSS',\n",
       "   'page_start': 1,\n",
       "   'page_end': 1,\n",
       "   'question': \"Hava Savunma S+stemler+ Yarışması'nın temel amacı ned+r?\",\n",
       "   'answer': 'Yarışmanın amacı, takımların ver+len senaryolara uygun görevler+ başarıyla yer+ne get+recek \\nhava savunma s+stemler+ gel+şt+rmes+ ve üretmes+d+r. Aynı zamanda, hava savunma \\ns+stemler+n+n önem+n+n ülke çapında gen+ş b+r tabana yayılarak özgün, yerl+ ve yetenekl+ \\ns+stemler+n gel+şt+r+lmes+n+ sağlamak da hedeﬂenmekted+r.'},\n",
       "  {'qa_id': 'HSS-Q002',\n",
       "   'competition': 'HSS',\n",
       "   'page_start': 1,\n",
       "   'page_end': 1,\n",
       "   'question': 'Yarışmaya k+mler katılab+l+r?',\n",
       "   'answer': \"Yarışmaya, Türk+ye'de veya yurt dışında öğren+m gören yükseköğret+m (ön l+sans, l+sans ve \\nyüksek l+sans) öğrenc+ler+ takım hal+nde başvuru yapab+lmekted+r.\"},\n",
       "  {'qa_id': 'HSS-Q003',\n",
       "   'competition': 'HSS',\n",
       "   'page_start': 1,\n",
       "   'page_end': 1,\n",
       "   'question': 'Takım oluşturma kuralları nelerd+r?',\n",
       "   'answer': 'Takımların en az 3, en fazla 15 k+ş+den oluşması zorunludur ve bu sayıya danışman dah+l \\ndeğ+ld+r. Takımlar, tek b+r okulun öğrenc+ler+nden oluşab+leceğ+ g+b+, farklı yükseköğret+m \\nkurumlarından öğrenc+ler+n b+r araya gelmes+yle karma olarak da oluşturulab+l+r.'}])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Topic labeling** (rule-based for now)",
   "id": "ed54c7c5f5b2aa60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create a simple keyword rule set; you can swap to ML later if needed.\n",
    "\n",
    "- **eligibility**: `katılım|kimler|başvuru koşul|uygun|gereklilik|şart`\n",
    "\n",
    "- **team**: `takım|üye|danışman|roller|katılımcı sayısı`\n",
    "\n",
    "- **stages**: `aşama|görev|süreç|workflow|sunum`\n",
    "\n",
    "- **scoring**: `puan|puanlama|BSP|bonus|değerlendir|kriter`\n",
    "\n",
    "- **penalties**: `ceza|diskalifiye|ihlal|yasak|kural dışı`\n",
    "\n",
    "- **logistics**: `konaklama|ulaşım|destek|sunum süresi|sponsor`\n",
    "\n",
    "- **timeline**: `son başvuru|tarih|takvim|deadline|program`\n",
    "\n",
    "\n",
    "Assign **the first matching topic**; if none match, label `other`. Keep it **overrideable** with a tiny YAML later."
   ],
   "id": "75f6b0499dddc249"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:15:30.415041Z",
     "start_time": "2025-08-14T11:15:30.404288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4) Topic labeling (simple rule-based for now) ---\n",
    "TOPIC_RULES = {\n",
    "    \"eligibility\": r\"(katılım|kimler|başvuru koşul|uygun|gereklilik|şart)\",\n",
    "    \"team\": r\"(takım|üye|danışman|rol|katılımcı sayısı)\",\n",
    "    \"stages\": r\"(aşama|görev|süreç|workflow|sunum)\",\n",
    "    \"scoring\": r\"(puan|puanlama|bsp|bonus|değerlendir|kriter)\",\n",
    "    \"penalties\": r\"(ceza|diskalifiye|ihlal|yasak|kural dışı|kuraldışı)\",\n",
    "    \"logistics\": r\"(konaklama|ulaşım|destek|sunum süresi|sponsor)\",\n",
    "    \"timeline\": r\"(son başvuru|tarih|takvim|deadline|program)\",\n",
    "}\n",
    "TOPIC_COMPILED = {k: re.compile(v, re.IGNORECASE) for k,v in TOPIC_RULES.items()}\n",
    "\n",
    "def label_topic(q, a):\n",
    "    text = (q + \"\\n\" + a).casefold()\n",
    "    for topic, rx in TOPIC_COMPILED.items():\n",
    "        if rx.search(text):\n",
    "            return topic\n",
    "    return \"other\"\n",
    "\n",
    "for qa in all_qas:\n",
    "    qa[\"topic\"] = label_topic(qa[\"question\"], qa[\"answer\"])\n"
   ],
   "id": "e18a5fcf4414c4e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:29:56.215215Z",
     "start_time": "2025-08-14T11:29:56.178044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TOPIC DISTRIBUTION: sanity check that key topics exist\n",
    "topic_counts = df[\"topic\"].value_counts()\n",
    "print(topic_counts)\n",
    "assert topic_counts.sum() == len(df), \"Topic labeling missing for some QAs.\"\n",
    "# Show one example per common topic\n",
    "for t in topic_counts.head(5).index:\n",
    "    row = df[df[\"topic\"]==t].iloc[0]\n",
    "    print(f\"\\nTopic={t} → {row['qa_id']}: {row['question']}\")\n"
   ],
   "id": "3d6b84ebe9622340",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic\n",
      "team           31\n",
      "other           7\n",
      "stages          5\n",
      "eligibility     4\n",
      "timeline        1\n",
      "scoring         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Topic=team → HSS-Q002: Yarışmaya k+mler katılab+l+r?\n",
      "\n",
      "Topic=other → HSS-Q006: Ön Tasarım Raporu (ÖTR) +ç+n son tesl+m tar+h+ ne zamandır?\n",
      "\n",
      "Topic=stages → HSS-Q016: Görev Kab+l+yet Göster+m+ V+deosu +ç+n çözünürlük ve süre gereks+n+mler+ nelerd+r?\n",
      "\n",
      "Topic=eligibility → HSS-Q001: Hava Savunma S+stemler+ Yarışması'nın temel amacı ned+r?\n",
      "\n",
      "Topic=timeline → HSS-Q005: Yarışma +ç+n son başvuru tar+h+ ned+r?\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Metadata extraction** (dates, numbers, formulas, links)",
   "id": "e36e3b9e46227d6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Dates**: handle Turkish and ISO; regex + `dateparser`:\n",
    "\n",
    "    - Match patterns like `\\b\\d{1,2}[./]\\d{1,2}[./]\\d{2,4}\\b`, or Turkish months:\n",
    "\n",
    "        - `Ocak|Şubat|Mart|Nisan|Mayıs|Haziran|Temmuz|Ağustos|Eylül|Ekim|Kasım|Aralık`\n",
    "\n",
    "    - Normalize to `YYYY-MM-DD` (store original + normalized).\n",
    "\n",
    "- **Numbers & units**:\n",
    "\n",
    "    - Percentages: `\\b\\d{1,3}\\s?%|\\%\\d{1,3}\\b`\n",
    "\n",
    "    - Minutes/seconds: `\\b\\d+\\s?(dk|dakika|sn|saniye)\\b`\n",
    "\n",
    "    - Team sizes: `\\b(en az|en fazla)\\s?\\d+\\b`\n",
    "\n",
    "    - Money: `\\b\\d{1,3}(\\.\\d{3})*(,\\d+)?\\s?(TL|₺|KZT|USD|EUR)\\b`\n",
    "\n",
    "- **Formulas**:\n",
    "\n",
    "    - Look for known tokens like `BSP` and capture math around them:\n",
    "\n",
    "        - e.g., `BSP\\s*[:=]\\s*([^\\n]+)` → store the formula string exactly.\n",
    "\n",
    "- **Links**:\n",
    "\n",
    "    - `https?://\\S+` (keep raw; you won’t auto-open in answers unless allowed).\n"
   ],
   "id": "622235eec89a706f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:17:40.703617Z",
     "start_time": "2025-08-14T11:17:40.649168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 5) Metadata extraction: dates, numbers, formulas, links ---\n",
    "MONTHS_TR = \"Ocak|Şubat|Mart|Nisan|Mayıs|Haziran|Temmuz|Ağustos|Eylül|Ekim|Kasım|Aralık\"\n",
    "DATE_RX = re.compile(\n",
    "    rf\"(\\b\\d{{1,2}}[./]\\d{{1,2}}[./]\\d{{2,4}}\\b|(\\b\\d{{1,2}}\\s+(?:{MONTHS_TR})\\s+\\d{{4}}\\b))\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "PERCENT_RX = re.compile(r\"\\b\\d{1,3}\\s?%|%\\s?\\d{1,3}\\b\")\n",
    "MINUTES_RX = re.compile(r\"\\b\\d+\\s?(?:dk|dakika)\\b\", re.IGNORECASE)\n",
    "POINTS_RX  = re.compile(r\"\\b(\\d+)\\s*puan\\b\", re.IGNORECASE)\n",
    "MONEY_RX   = re.compile(r\"\\b\\d{1,3}(?:\\.\\d{3})*(?:,\\d+)?\\s?(?:TL|₺|KZT|USD|EUR)\\b\")\n",
    "BSP_RX     = re.compile(r\"\\bBSP\\b.*\", re.IGNORECASE)\n",
    "LINK_RX    = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def extract_dates_tr(text: str):\n",
    "    \"\"\"Return list of {raw, iso} using dateparser (TR) with regex pre-filter.\"\"\"\n",
    "    hits = []\n",
    "    for m in DATE_RX.finditer(text):\n",
    "        raw = m.group(0)\n",
    "        # dateparser search (language TR)\n",
    "        found = search_dates(raw, languages=[\"tr\"], settings={\"DATE_ORDER\": \"DMY\"})\n",
    "        if found:\n",
    "            # found is list of tuples (raw, datetime)\n",
    "            for _, dt in found:\n",
    "                iso = dt.strftime(\"%Y-%m-%d\")\n",
    "                hits.append({\"raw\": raw, \"iso\": iso})\n",
    "        else:\n",
    "            hits.append({\"raw\": raw, \"iso\": None})\n",
    "    # deduplicate by (raw, iso)\n",
    "    uniq = {(h[\"raw\"], h[\"iso\"]): h for h in hits}\n",
    "    return list(uniq.values())\n",
    "\n",
    "def extract_numbers(text: str):\n",
    "    nums = []\n",
    "    for rx, kind in [(PERCENT_RX, \"percent\"), (MINUTES_RX, \"minutes\"), (POINTS_RX, \"points\"), (MONEY_RX, \"money\")]:\n",
    "        for m in rx.finditer(text):\n",
    "            nums.append({\"raw\": m.group(0), \"kind\": kind})\n",
    "    return nums\n",
    "\n",
    "def extract_formulas(text: str):\n",
    "    return [{\"name\": \"BSP\", \"expr\": line.strip()} for line in BSP_RX.findall(text)]\n",
    "\n",
    "def extract_links(text: str):\n",
    "    return LINK_RX.findall(text)\n",
    "\n",
    "def detect_stage(text: str):\n",
    "    m = re.search(r\"aşama\\s*(\\d)\", text, re.IGNORECASE)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "for qa in all_qas:\n",
    "    full = qa[\"question\"] + \"\\n\" + qa[\"answer\"]\n",
    "    qa[\"stage\"]    = detect_stage(full)\n",
    "    qa[\"dates\"]    = extract_dates_tr(full)\n",
    "    qa[\"numbers\"]  = extract_numbers(full)\n",
    "    qa[\"formulas\"] = extract_formulas(full)\n",
    "    qa[\"links\"]    = extract_links(full)\n"
   ],
   "id": "290f451878766087",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:30:30.106513Z",
     "start_time": "2025-08-14T11:30:30.095517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# METADATA COVERAGE: how many QAs have dates/numbers/formulas/links\n",
    "has_dates    = sum(bool(x) for x in df[\"dates\"])\n",
    "has_numbers  = sum(bool(x) for x in df[\"numbers\"])\n",
    "has_formulas = sum(bool(x) for x in df[\"formulas\"])\n",
    "has_links    = sum(bool(x) for x in df[\"links\"])\n",
    "print(f\"Dates:{has_dates}  Numbers:{has_numbers}  Formulas:{has_formulas}  Links:{has_links}\")\n",
    "\n",
    "# Spot-check BSP presence if expected\n",
    "bsp_rows = df[df[\"formulas\"].astype(str).str.contains(\"BSP\", case=False, na=False)]\n",
    "print(\"BSP rows:\", len(bsp_rows))\n",
    "if len(bsp_rows):\n",
    "    print(\"Example BSP expr:\", bsp_rows.iloc[0][\"formulas\"][0])"
   ],
   "id": "b97163f4b80ddae6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates:4  Numbers:11  Formulas:1  Links:1\n",
      "BSP rows: 1\n",
      "Example BSP expr: {'name': 'BSP', 'expr': 'BSP)'}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Output artifacts**",
   "id": "a3d8b23b0576e630"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Markdown corpus** (for human review + optional BM25 later):\n",
    "\n",
    "    - One file per competition, e.g., `data/processed/md/HSS.md`\n",
    "\n",
    "    - Format:\n",
    "\n",
    "        ```\n",
    "        # Hava Savunma Sistemleri\n",
    "\n",
    "        ## Q23. <question text>\n",
    "        <answer paragraph(s)>\n",
    "\n",
    "        [page: 7–8] [qa_id: HSS-Q023]\n",
    "        ---\n",
    "        ```\n",
    "\n",
    "2. **JSONL sidecar** with one object per Q&A:\n",
    "\n",
    "    - `data/processed/jsonl/qa_meta.jsonl`\n",
    "\n",
    "    - Fields: `qa_id, competition, topic, stage, page_start, page_end, question, answer, dates, numbers, formulas, links`\n",
    "\n",
    "3. **CSVs (optional, for exact lookups)**\n",
    "\n",
    "    - `deadlines.csv` columns: `competition, label, date_iso, page_start, qa_id`\n",
    "\n",
    "    - `scoring.csv` columns: `competition, item, points, penalties, formula, qa_id`"
   ],
   "id": "504c394ddcd17a0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:19:14.340177Z",
     "start_time": "2025-08-14T11:19:14.332340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 6) Write Markdown per competition ---\n",
    "OUT_MD.mkdir(parents=True, exist_ok=True)\n",
    "grouped = {}\n",
    "for qa in all_qas:\n",
    "    grouped.setdefault(qa[\"competition\"], []).append(qa)\n",
    "\n",
    "for comp, items in grouped.items():\n",
    "    items = sorted(items, key=lambda x: x[\"qa_id\"])\n",
    "    lines = [f\"# {comp}\\n\"]\n",
    "    for it in items:\n",
    "        lines.append(f\"## {it['qa_id']}: {it['question']}\")\n",
    "        lines.append(it[\"answer\"].rstrip())\n",
    "        lines.append(f\"[page: {it['page_start']}–{it['page_end']}] [topic: {it['topic']}]\")\n",
    "        lines.append(\"---\\n\")\n",
    "    (OUT_MD / f\"{comp}.md\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")"
   ],
   "id": "ffd74cd0f8fb26b6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:19:19.808840Z",
     "start_time": "2025-08-14T11:19:19.798414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 7) Write JSONL sidecar with rich metadata ---\n",
    "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in all_qas:\n",
    "        rec = {\n",
    "            \"qa_id\": qa[\"qa_id\"],\n",
    "            \"competition\": qa[\"competition\"],\n",
    "            \"topic\": qa[\"topic\"],\n",
    "            \"stage\": qa[\"stage\"],\n",
    "            \"page_start\": qa[\"page_start\"],\n",
    "            \"page_end\": qa[\"page_end\"],\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answer\": qa[\"answer\"],\n",
    "            \"dates\": qa[\"dates\"],\n",
    "            \"numbers\": qa[\"numbers\"],\n",
    "            \"formulas\": qa[\"formulas\"],\n",
    "            \"links\": qa[\"links\"],\n",
    "        }\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n"
   ],
   "id": "53d113b08b4fe935",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:19:31.360137Z",
     "start_time": "2025-08-14T11:19:31.352155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 7) Write JSONL sidecar with rich metadata ---\n",
    "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in all_qas:\n",
    "        rec = {\n",
    "            \"qa_id\": qa[\"qa_id\"],\n",
    "            \"competition\": qa[\"competition\"],\n",
    "            \"topic\": qa[\"topic\"],\n",
    "            \"stage\": qa[\"stage\"],\n",
    "            \"page_start\": qa[\"page_start\"],\n",
    "            \"page_end\": qa[\"page_end\"],\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answer\": qa[\"answer\"],\n",
    "            \"dates\": qa[\"dates\"],\n",
    "            \"numbers\": qa[\"numbers\"],\n",
    "            \"formulas\": qa[\"formulas\"],\n",
    "            \"links\": qa[\"links\"],\n",
    "        }\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n"
   ],
   "id": "1347772347eb299",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:19:43.695461Z",
     "start_time": "2025-08-14T11:19:43.677377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 8) Optional CSVs: deadlines & scoring (simple first cut) ---\n",
    "# Deadlines: any QA with dates (prefer those with timeline topic)\n",
    "deadline_rows = []\n",
    "for qa in all_qas:\n",
    "    if not qa[\"dates\"]:\n",
    "        continue\n",
    "    label = \"timeline\" if qa[\"topic\"] == \"timeline\" else \"general\"\n",
    "    for d in qa[\"dates\"]:\n",
    "        deadline_rows.append({\n",
    "            \"competition\": qa[\"competition\"],\n",
    "            \"qa_id\": qa[\"qa_id\"],\n",
    "            \"label\": label,\n",
    "            \"date_raw\": d[\"raw\"],\n",
    "            \"date_iso\": d[\"iso\"],\n",
    "            \"page_start\": qa[\"page_start\"],\n",
    "        })\n",
    "pd.DataFrame(deadline_rows).to_csv(OUT_DEADLINES, index=False)\n",
    "\n",
    "# Scoring: capture points, BSP formulas, and % mentions\n",
    "scoring_rows = []\n",
    "for qa in all_qas:\n",
    "    if qa[\"topic\"] not in (\"scoring\", \"penalties\") and not qa[\"formulas\"]:\n",
    "        continue\n",
    "    pts = [n[\"raw\"] for n in qa[\"numbers\"] if n[\"kind\"] in (\"points\",\"percent\")]\n",
    "    scoring_rows.append({\n",
    "        \"competition\": qa[\"competition\"],\n",
    "        \"qa_id\": qa[\"qa_id\"],\n",
    "        \"points_mentions\": \"; \".join(pts) if pts else \"\",\n",
    "        \"formulas\": \"; \".join([f[\"expr\"] for f in qa[\"formulas\"]]) if qa[\"formulas\"] else \"\",\n",
    "        \"page_start\": qa[\"page_start\"],\n",
    "    })\n",
    "pd.DataFrame(scoring_rows).to_csv(OUT_SCORING, index=False)\n",
    "\n",
    "print(\"Split & structure complete →\",\n",
    "      \"\\n- data/processed/md/{HSS,E-TICARET,ADRES}.md\",\n",
    "      \"\\n- data/processed/jsonl/qa_meta.jsonl\",\n",
    "      \"\\n- data/processed/csv/deadlines.csv (optional)\",\n",
    "      \"\\n- data/processed/csv/scoring.csv (optional)\")\n"
   ],
   "id": "ec9761a7a8ad8630",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split & structure complete → \n",
      "- data/processed/md/{HSS,E-TICARET,ADRES}.md \n",
      "- data/processed/jsonl/qa_meta.jsonl \n",
      "- data/processed/csv/deadlines.csv (optional) \n",
      "- data/processed/csv/scoring.csv (optional)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:31:41.126473Z",
     "start_time": "2025-08-14T11:31:41.110395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CSV SHAPES: optional but useful quick checks\n",
    "if OUT_DEADLINES.exists():\n",
    "    dfd = pd.read_csv(OUT_DEADLINES)\n",
    "    print(\"deadlines.csv →\", dfd.shape, \"| examples:\", dfd.head(2).to_dict(\"records\"))\n",
    "if OUT_SCORING.exists():\n",
    "    dfs = pd.read_csv(OUT_SCORING)\n",
    "    print(\"scoring.csv →\", dfs.shape, \"| examples:\", dfs.head(2).to_dict(\"records\"))\n"
   ],
   "id": "f22738b7210073f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deadlines.csv → (4, 6) | examples: [{'competition': 'HSS', 'qa_id': 'HSS-Q005', 'label': 'timeline', 'date_raw': '1 Mart 2025', 'date_iso': '2025-03-01', 'page_start': 1}, {'competition': 'HSS', 'qa_id': 'HSS-Q006', 'label': 'general', 'date_raw': '17 Mart 2025', 'date_iso': '2025-03-17', 'page_start': 1}]\n",
      "scoring.csv → (2, 5) | examples: [{'competition': 'HSS', 'qa_id': 'HSS-Q022', 'points_mentions': '40 puan; 0 puan', 'formulas': nan, 'page_start': 4}, {'competition': 'HSS', 'qa_id': 'HSS-Q050', 'points_mentions': '60 puan; 100 puan; 140 puan; 0 puan; 5 puan; 20 puan; 10 Puan; 10 Puan; 10 \\npuan', 'formulas': 'BSP); BSP), Hava Savunma Sistemleri yarışmasının her üç görev aşamasında', 'page_start': 8}]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Quality gates** (small, but critical)",
   "id": "eae41c89a067b016"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **No-empty**: Assert question and answer are non-empty for all items.\n",
    "\n",
    "- **Length bounds**: Flag any answer > 2,000 chars (may indicate merge error).\n",
    "\n",
    "- **Competition consistency**: All Q&As must inherit a competition. If any `None`, inspect page anchors.\n",
    "\n",
    "- **Duplicate detection**: Use `rapidfuzz` to flag 95%+ near-duplicates; collapse if needed."
   ],
   "id": "b92b17359f2da1c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T11:20:34.269090Z",
     "start_time": "2025-08-14T11:20:34.234001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 9) Minimal quality gates (fail early if something is off) ---\n",
    "import sys\n",
    "\n",
    "df = pd.DataFrame(all_qas)\n",
    "assert not df[\"question\"].isna().any() and not df[\"answer\"].isna().any(), \"Empty Q/A found\"\n",
    "too_long = df[\"answer\"].str.len() > 2000\n",
    "if too_long.any():\n",
    "    print(\"WARNING: Very long answers detected:\\n\", df[too_long][[\"qa_id\",\"competition\"]].head())\n",
    "if df[\"competition\"].isna().any():\n",
    "    raise RuntimeError(\"Some QAs have no competition assigned\")\n",
    "dups = df[\"question\"].duplicated(keep=False)\n",
    "if dups.any():\n",
    "    print(\"NOTE: Near duplicates detected; consider post-processing:\\n\", df[dups][[\"qa_id\",\"question\"]].head())\n",
    "print(\"Quality checks passed.\")\n"
   ],
   "id": "7b4041bfa2c12830",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Very long answers detected:\n",
      "        qa_id competition\n",
      "48  HSS-Q050         HSS\n",
      "Quality checks passed.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e6570e1de5bf56fa"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
