{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook 01 — Ingestion & Structure\n",
    "Goal: extract PDF → normalize Turkish text → map pages to competitions → prepare linearized MD."
   ],
   "id": "46f00d8566d2411a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T13:33:38.535990Z",
     "start_time": "2025-08-14T13:33:38.502143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 0) Setup & paths ---\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolve project root (assuming notebooks/ is under project root)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT = NOTEBOOK_DIR.parent\n",
    "# Standard data layout\n",
    "RAW_DIR        = ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR  = ROOT / \"data\" / \"processed\"\n",
    "MD_DIR         = PROCESSED_DIR / \"md\"\n",
    "JSONL_DIR      = PROCESSED_DIR / \"jsonl\"\n",
    "CSV_DIR        = PROCESSED_DIR / \"csv\"\n",
    "\n",
    "# Ensure output dirs exist\n",
    "for d in (RAW_DIR, PROCESSED_DIR, MD_DIR, JSONL_DIR, CSV_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Candidate PDF locations (put the file in data/raw as rag-example-qa.pdf)\n",
    "PDF_CANDIDATES = [\n",
    "    RAW_DIR / \"rag-example-qa.pdf\",\n",
    "    ROOT / \"rag-example-qa.pdf\",            # fallback if you keep it at project root\n",
    "]\n",
    "\n",
    "pdf_path = next((p for p in PDF_CANDIDATES if p.exists()), None)\n",
    "if pdf_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"PDF not found. Place your file at: {RAW_DIR / 'rag-example-qa.pdf'} and re-run this cell.\"\n",
    "    )\n",
    "\n",
    "# Declare outputs we’ll write in later cells\n",
    "OUT_PAGE_JSONL     = PROCESSED_DIR / \"page_dump.jsonl\"\n",
    "OUT_PAGE_SECTIONS  = PROCESSED_DIR / \"page_sections.csv\"\n",
    "OUT_RAW_LINEAR_MD  = MD_DIR / \"raw_linearized.md\"\n",
    "\n",
    "print(\"Project ROOT     :\", ROOT)\n",
    "print(\"Using PDF        :\", pdf_path)\n",
    "print(\"Outputs will be  :\", OUT_PAGE_JSONL, OUT_PAGE_SECTIONS, OUT_RAW_LINEAR_MD, sep=\"\\n  - \")"
   ],
   "id": "4772cff9cdcd203b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ROOT     : /Users/macbook/T3/rag-tekno\n",
      "Using PDF        : /Users/macbook/T3/rag-tekno/data/raw/rag-example-qa.pdf\n",
      "Outputs will be  :\n",
      "  - /Users/macbook/T3/rag-tekno/data/processed/page_dump.jsonl\n",
      "  - /Users/macbook/T3/rag-tekno/data/processed/page_sections.csv\n",
      "  - /Users/macbook/T3/rag-tekno/data/processed/md/raw_linearized.md\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extract text page-by-page (writes page_dump.jsonl)",
   "id": "a99c17e9a4f55160"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T13:35:48.497689Z",
     "start_time": "2025-08-14T13:35:48.384366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1) Extract text page-by-page ---\n",
    "import fitz  # PyMuPDF\n",
    "import json, statistics as stats\n",
    "\n",
    "pages = []\n",
    "with fitz.open(pdf_path) as doc:\n",
    "    for i, page in enumerate(doc, start=1):\n",
    "        text = page.get_text(\"text\")  # preserves reading order reasonably\n",
    "        pages.append({\"page\": i, \"text\": text})\n",
    "\n",
    "# persist raw page dump (one JSON per line)\n",
    "with open(OUT_PAGE_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in pages:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# quick checks\n",
    "lengths = [len(p[\"text\"]) for p in pages]\n",
    "print(f\"Pages extracted: {len(pages)}\")\n",
    "print(f\"Avg chars/page: {int(stats.mean(lengths))} | min: {min(lengths)} | max: {max(lengths)}\")\n",
    "print(\"Preview p1:\\n\", pages[0][\"text\"][:400])"
   ],
   "id": "9e6e162e6a17e8f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages extracted: 28\n",
      "Avg chars/page: 2324 | min: 1475 | max: 3035\n",
      "Preview p1:\n",
      " Hava Savunma S)stemler) Yarışması \n",
      "1. Hava Savunma S+stemler+ Yarışması'nın temel amacı ned+r? \n",
      "Yarışmanın amacı, takımların ver+len senaryolara uygun görevler+ başarıyla yer+ne get+recek \n",
      "hava savunma s+stemler+ gel+şt+rmes+ ve üretmes+d+r. Aynı zamanda, hava savunma \n",
      "s+stemler+n+n önem+n+n ülke çapında gen+ş b+r tabana yayılarak özgün, yerl+ ve yetenekl+ \n",
      "s+stemler+n gel+şt+r+lmes+n+ sağlamak da\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ## Normalize Turkish text (dehyphen, newline cleanup) + quick checks\n",
    "\n",
    "Purpose: create a clean text_norm per page; verify dehyphenation improved things."
   ],
   "id": "27d12996d9c02411"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T13:37:50.543986Z",
     "start_time": "2025-08-14T13:37:50.511055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2) Normalize Turkish text (idempotent) ---\n",
    "import re\n",
    "\n",
    "SOFT_HYPHEN = \"\\u00ad\"\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # remove soft hyphens / tabs\n",
    "    s = s.replace(SOFT_HYPHEN, \"\").replace(\"\\t\", \" \")\n",
    "    # dehyphenate across line breaks: \"kelime-\\nler\" -> \"kelimeler\"\n",
    "    s = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", s)\n",
    "    # unify newlines\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # collapse 3+ newlines to 2\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    # ensure numbered lists have a space after \"1.\" etc\n",
    "    s = re.sub(r\"(?m)^(\\s*\\d+)\\.(\\S)\", r\"\\1. \\2\", s)\n",
    "    return s\n",
    "\n",
    "# apply + quick health checks\n",
    "hyphen_breaks_before = sum(len(re.findall(r\"(\\w+)-\\n(\\w+)\", p[\"text\"])) for p in pages)\n",
    "for p in pages:\n",
    "    p[\"text_norm\"] = normalize_text(p[\"text\"])\n",
    "hyphen_breaks_after  = sum(len(re.findall(r\"(\\w+)-\\n(\\w+)\", p[\"text_norm\"])) for p in pages)\n",
    "\n",
    "print(f\"Dehyphenation: before={hyphen_breaks_before} → after={hyphen_breaks_after}\")\n",
    "print(\"Normalized preview p1:\\n\", pages[0][\"text_norm\"][:400])"
   ],
   "id": "a46d78698a213190",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dehyphenation: before=3 → after=0\n",
      "Normalized preview p1:\n",
      " Hava Savunma S)stemler) Yarışması \n",
      "1. Hava Savunma S+stemler+ Yarışması'nın temel amacı ned+r? \n",
      "Yarışmanın amacı, takımların ver+len senaryolara uygun görevler+ başarıyla yer+ne get+recek \n",
      "hava savunma s+stemler+ gel+şt+rmes+ ve üretmes+d+r. Aynı zamanda, hava savunma \n",
      "s+stemler+n+n önem+n+n ülke çapında gen+ş b+r tabana yayılarak özgün, yerl+ ve yetenekl+ \n",
      "s+stemler+n gel+şt+r+lmes+n+ sağlamak da\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Detect competition per page (robust anchors + fill) — brief\n",
    "Purpose: tag each page as HSS, E-TICARET, or ADRES via tolerant matching (handles hyphen/spacing/diacritics), then back/forward-fill gaps. Prints counts and boundary pages."
   ],
   "id": "f9385579b0cf1de5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T13:40:38.480606Z",
     "start_time": "2025-08-14T13:40:38.433400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3) Detect competition per page (robust anchors) ---\n",
    "import re, unicodedata, pandas as pd\n",
    "\n",
    "def norm_for_match(s: str) -> str:\n",
    "    # strip diacritics, lowercase, collapse hyphens/punct to spaces\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[-_./]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "ANCHORS = {\n",
    "    \"HSS\": [\n",
    "        r\"\\bhava savunma sistemleri\\b\",\n",
    "        r\"\\bhss\\b\",\n",
    "    ],\n",
    "    \"E-TICARET\": [\n",
    "        r\"\\be ticaret hackathonu\\b\",\n",
    "        r\"\\be ticaret\\b\",\n",
    "        r\"\\beticaret\\b\",\n",
    "    ],\n",
    "    \"ADRES\": [\n",
    "        r\"\\byapay zeka destekli adres cozumleme\\b\",\n",
    "        r\"\\byapay zeka destekli adres\\b\",\n",
    "        r\"\\badres cozumleme\\b\",\n",
    "    ],\n",
    "}\n",
    "ANCHORS_RE = {code: [re.compile(p) for p in pats] for code, pats in ANCHORS.items()}\n",
    "\n",
    "def detect_competitions(pages):\n",
    "    last_seen = None\n",
    "    comp_by_page = {}\n",
    "    # pass 1: look for explicit anchors\n",
    "    for p in pages:\n",
    "        t_norm = norm_for_match(p[\"text_norm\"])\n",
    "        found = None\n",
    "        for code, patterns in ANCHORS_RE.items():\n",
    "            if any(rx.search(t_norm) for rx in patterns):\n",
    "                found = code\n",
    "                break\n",
    "        last_seen = found or last_seen\n",
    "        comp_by_page[p[\"page\"]] = last_seen  # may be None initially\n",
    "\n",
    "    # pass 2: back-fill from next known\n",
    "    keys = sorted(comp_by_page)\n",
    "    next_seen = None\n",
    "    for k in reversed(keys):\n",
    "        if comp_by_page[k] is None and next_seen is not None:\n",
    "            comp_by_page[k] = next_seen\n",
    "        elif comp_by_page[k] is not None:\n",
    "            next_seen = comp_by_page[k]\n",
    "\n",
    "    # pass 3: forward-fill from previous known\n",
    "    prev_seen = None\n",
    "    for k in keys:\n",
    "        if comp_by_page[k] is None and prev_seen is not None:\n",
    "            comp_by_page[k] = prev_seen\n",
    "        elif comp_by_page[k] is not None:\n",
    "            prev_seen = comp_by_page[k]\n",
    "    return comp_by_page\n",
    "\n",
    "comp_by_page = detect_competitions(pages)\n",
    "\n",
    "df_comp = pd.DataFrame({\n",
    "    \"page\": [p[\"page\"] for p in pages],\n",
    "    \"competition\": [comp_by_page[p[\"page\"]] for p in pages]\n",
    "})\n",
    "\n",
    "print(\"by competition:\\n\", df_comp[\"competition\"].value_counts(dropna=False))\n",
    "transitions = df_comp[df_comp[\"competition\"].shift(1) != df_comp[\"competition\"]]\n",
    "print(\"\\nDetected competition boundaries (first few):\\n\", transitions.head(10))"
   ],
   "id": "3c394ebc66cef352",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by competition:\n",
      " competition\n",
      "HSS          14\n",
      "ADRES        10\n",
      "E-TICARET     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Detected competition boundaries (first few):\n",
      "     page competition\n",
      "0      1         HSS\n",
      "14    15   E-TICARET\n",
      "18    19       ADRES\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Save page→competition map (CSV)\n",
    "Purpose: persist the routing we just detected for downstream notebooks."
   ],
   "id": "2b08a2c18b32ba7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T13:42:38.077241Z",
     "start_time": "2025-08-14T13:42:38.046077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4) Persist page→competition map ---\n",
    "import pandas as pd\n",
    "\n",
    "df_comp = pd.DataFrame({\n",
    "    \"page\": [p[\"page\"] for p in pages],\n",
    "    \"competition\": [comp_by_page[p[\"page\"]] for p in pages]\n",
    "})\n",
    "df_comp.to_csv(OUT_PAGE_SECTIONS, index=False, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", OUT_PAGE_SECTIONS, \"| rows:\", len(df_comp))\n",
    "print(df_comp.head(28))"
   ],
   "id": "c13179ab125144c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/macbook/T3/rag-tekno/data/processed/page_sections.csv | rows: 28\n",
      "    page competition\n",
      "0      1         HSS\n",
      "1      2         HSS\n",
      "2      3         HSS\n",
      "3      4         HSS\n",
      "4      5         HSS\n",
      "5      6         HSS\n",
      "6      7         HSS\n",
      "7      8         HSS\n",
      "8      9         HSS\n",
      "9     10         HSS\n",
      "10    11         HSS\n",
      "11    12         HSS\n",
      "12    13         HSS\n",
      "13    14         HSS\n",
      "14    15   E-TICARET\n",
      "15    16   E-TICARET\n",
      "16    17   E-TICARET\n",
      "17    18   E-TICARET\n",
      "18    19       ADRES\n",
      "19    20       ADRES\n",
      "20    21       ADRES\n",
      "21    22       ADRES\n",
      "22    23       ADRES\n",
      "23    24       ADRES\n",
      "24    25       ADRES\n",
      "25    26       ADRES\n",
      "26    27       ADRES\n",
      "27    28       ADRES\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Linearized Markdown (one block per page)\n",
    "Purpose: quick, human-readable file for spot checks and future regex work."
   ],
   "id": "dc04ac617084c824"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T13:43:09.096027Z",
     "start_time": "2025-08-14T13:43:09.071818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 5) Write linearized Markdown (per page with headers) ---\n",
    "lines = []\n",
    "for p in pages:\n",
    "    comp = comp_by_page[p[\"page\"]] or \"UNKNOWN\"\n",
    "    lines.append(f\"# PAGE {p['page']} | {comp}\")\n",
    "    lines.append(\"\")  # blank line\n",
    "    lines.append(p[\"text_norm\"].rstrip())\n",
    "    lines.append(\"\\n---\\n\")  # page separator\n",
    "\n",
    "md_text = \"\\n\".join(lines)\n",
    "MD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_RAW_LINEAR_MD.write_text(md_text, encoding=\"utf-8\")\n",
    "\n",
    "# quick checks\n",
    "print(\"Wrote:\", OUT_RAW_LINEAR_MD, \"| chars:\", len(md_text))\n",
    "print(\"Anchors present:\",\n",
    "      \"PAGE 1\" in md_text,\n",
    "      \"E-TICARET\" in md_text,\n",
    "      \"ADRES\" in md_text)\n",
    "print(\"\\nPreview:\\n\", md_text.splitlines()[:12])"
   ],
   "id": "f7cc5b98e5af97d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/macbook/T3/rag-tekno/data/processed/md/raw_linearized.md | chars: 65709\n",
      "Anchors present: True True True\n",
      "\n",
      "Preview:\n",
      " ['# PAGE 1 | HSS', '', 'Hava Savunma S)stemler) Yarışması ', \"1. Hava Savunma S+stemler+ Yarışması'nın temel amacı ned+r? \", 'Yarışmanın amacı, takımların ver+len senaryolara uygun görevler+ başarıyla yer+ne get+recek ', 'hava savunma s+stemler+ gel+şt+rmes+ ve üretmes+d+r. Aynı zamanda, hava savunma ', 's+stemler+n+n önem+n+n ülke çapında gen+ş b+r tabana yayılarak özgün, yerl+ ve yetenekl+ ', 's+stemler+n gel+şt+r+lmes+n+ sağlamak da hedeﬂenmekted+r. ', ' ', '2. Yarışmaya k+mler katılab+l+r? ', \"Yarışmaya, Türk+ye'de veya yurt dışında öğren+m gören yükseköğret+m (ön l+sans, l+sans ve \", 'yüksek l+sans) öğrenc+ler+ takım hal+nde başvuru yapab+lmekted+r. ']\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T13:51:58.062872Z",
     "start_time": "2025-08-14T13:51:57.996873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 6) Final sanity checks ---\n",
    "import json, pandas as pd\n",
    "\n",
    "# page dump lines == 28\n",
    "line_count = sum(1 for _ in open(OUT_PAGE_JSONL, \"r\", encoding=\"utf-8\"))\n",
    "print(\"page_dump.jsonl lines:\", line_count)\n",
    "\n",
    "# page→competition rows == 28 and all labeled\n",
    "df_map = pd.read_csv(OUT_PAGE_SECTIONS)\n",
    "print(\"page_sections rows:\", len(df_map))\n",
    "print(df_map[\"competition\"].value_counts(dropna=False))\n",
    "\n",
    "assert line_count == 28 and len(df_map) == 28\n",
    "assert df_map[\"competition\"].isna().sum() == 0\n",
    "print(\"Notebook 01 ✔️ complete\")"
   ],
   "id": "f40a4b3a8c8bbed6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_dump.jsonl lines: 28\n",
      "page_sections rows: 28\n",
      "competition\n",
      "HSS          14\n",
      "ADRES        10\n",
      "E-TICARET     4\n",
      "Name: count, dtype: int64\n",
      "Notebook 01 ✔️ complete\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "922bea22cac00989"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
