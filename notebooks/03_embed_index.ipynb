{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook 03 — Embeddings (E5) & FAISS index\n",
    "Goal: build one-QA-per-chunk embeddings with E5 (L2-normalized) and store in FAISS + Parquet metadata."
   ],
   "id": "b74f723a36cdd8ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup & load sidecar\n",
    "\n",
    "Purpose: load qa_meta.jsonl, create stable row ids, and prep passage strings for E5."
   ],
   "id": "e880d625cbe03b26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:08:59.429311Z",
     "start_time": "2025-08-14T18:08:59.171971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 0) Setup & load ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "PROCESSED_DIR = ROOT / \"data\" / \"processed\"\n",
    "INDEX_DIR     = ROOT / \"data\" / \"index\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SIDEcar = PROCESSED_DIR / \"jsonl\" / \"qa_meta.jsonl\"\n",
    "assert SIDEcar.exists(), f\"Missing {SIDEcar}\"\n",
    "\n",
    "df = pd.read_json(SIDEcar, lines=True)\n",
    "# stable row id aligned with embedding order\n",
    "df = df.reset_index(drop=True).rename_axis(\"rid\").reset_index()\n",
    "# E5 passage text (keep Q + A together)\n",
    "df[\"passage\"] = \"passage: \" + df[\"question\"].astype(str).str.strip() + \"\\n\" + df[\"answer\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", list(df.columns)[:10], \"…\")\n",
    "print(df[[\"rid\",\"qa_id\",\"competition\",\"topic\",\"page_start\",\"page_end\"]].head(3))"
   ],
   "id": "d9571a2c1f4e56e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 102\n",
      "Columns: ['rid', 'qa_id', 'competition', 'topic', 'stage', 'page_start', 'page_end', 'question', 'answer', 'dates'] …\n",
      "   rid        qa_id competition        topic  page_start  page_end\n",
      "0    0  ADRES-QG001       ADRES  eligibility          19        19\n",
      "1    1  ADRES-QG002       ADRES  eligibility          19        19\n",
      "2    2  ADRES-QG003       ADRES  eligibility          19        19\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Build chunks from sidecar (QA + micro-chunks)\n",
    "\n",
    "Purpose: create meta with content strings ready for embedding (no model yet)."
   ],
   "id": "35910f883db3d8be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:15:27.278104Z",
     "start_time": "2025-08-14T18:15:27.152003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1) Build chunks (1 main chunk per QA + tiny micro-chunks) ---\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "rid = 0\n",
    "\n",
    "for row in df.itertuples(index=False):\n",
    "    # main chunk: atomic Q&A\n",
    "    content_main = f\"Q: {str(row.question).strip()}\\nA: {str(row.answer).strip()}\"\n",
    "    records.append({\n",
    "        \"rid\": rid, \"qa_id\": row.qa_id, \"competition\": row.competition, \"topic\": row.topic,\n",
    "        \"stage\": row.stage, \"page_start\": row.page_start, \"page_end\": row.page_end,\n",
    "        \"chunk_type\": \"qa\", \"content\": content_main\n",
    "    }); rid += 1\n",
    "\n",
    "    # micro-chunks: formulas (BSP/equations) — in this dataset they are plain strings\n",
    "    for f in (row.formulas or []):\n",
    "        f_str = (f or \"\").strip()\n",
    "        if f_str:\n",
    "            records.append({\n",
    "                \"rid\": rid, \"qa_id\": row.qa_id, \"competition\": row.competition, \"topic\": \"scoring\",\n",
    "                \"stage\": row.stage, \"page_start\": row.page_start, \"page_end\": row.page_end,\n",
    "                \"chunk_type\": \"formula\", \"content\": f\"Formula: {f_str}\"\n",
    "            }); rid += 1\n",
    "\n",
    "    # micro-chunks: limits/dates (compact retrieval signals)\n",
    "    nums_list = list({(n if isinstance(n, str) else \" \".join(n)) for n in (row.numbers or [])})\n",
    "    # dates are dicts like {\"raw\": \"...\", \"iso\": \"...\"}; keep raw for retrieval\n",
    "    raw_dates = []\n",
    "    for d in (row.dates or []):\n",
    "        if isinstance(d, dict) and \"raw\" in d:\n",
    "            raw_dates.append(d[\"raw\"])\n",
    "        elif isinstance(d, str):\n",
    "            raw_dates.append(d)\n",
    "    raw_dates = list(set(raw_dates))\n",
    "\n",
    "    if nums_list or raw_dates:\n",
    "        compact = \" | \".join(\n",
    "            [ \"; \".join(sorted(nums_list)) ] if raw_dates == [] else\n",
    "            [ \"; \".join(sorted(nums_list)) , \"; \".join(sorted(raw_dates)) ] if nums_list else\n",
    "            [ \"; \".join(sorted(raw_dates)) ]\n",
    "        )\n",
    "        records.append({\n",
    "            \"rid\": rid, \"qa_id\": row.qa_id, \"competition\": row.competition, \"topic\": row.topic,\n",
    "            \"stage\": row.stage, \"page_start\": row.page_start, \"page_end\": row.page_end,\n",
    "            \"chunk_type\": \"limits\", \"content\": compact\n",
    "        }); rid += 1\n",
    "\n",
    "meta = pd.DataFrame.from_records(records)\n",
    "print(len(meta), meta[\"chunk_type\"].value_counts())"
   ],
   "id": "53de2ee3732ea342",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 chunk_type\n",
      "qa         102\n",
      "limits      26\n",
      "formula      9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embedding config & deps\n",
    "\n",
    "Purpose: set model id, batch size, device; import libs; define output paths."
   ],
   "id": "f732afe6f2ec374e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:17:10.707451Z",
     "start_time": "2025-08-14T18:17:10.697005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2) Embedding config & deps ---\n",
    "import numpy as np\n",
    "import torch, faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMBED_MODEL_ID = \"intfloat/multilingual-e5-base\"  # good TR/EN quality\n",
    "BATCH_SIZE = 24                                    # safe for M2/8GB; raise if it feels comfy\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "META_PATH  = INDEX_DIR / \"meta.parquet\"\n",
    "FAISS_PATH = INDEX_DIR / \"faiss_e5.index\"\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Model:\", EMBED_MODEL_ID)\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Meta path:\", META_PATH)\n",
    "print(\"FAISS path:\", FAISS_PATH)"
   ],
   "id": "c2e01eb4e2386223",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Model: intfloat/multilingual-e5-base\n",
      "Batch size: 24\n",
      "Meta path: /Users/macbook/T3/rag-tekno/data/index/meta.parquet\n",
      "FAISS path: /Users/macbook/T3/rag-tekno/data/index/faiss_e5.index\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load E5 model + tiny probe\n",
    "\n",
    "Purpose: make sure the model runs on your device."
   ],
   "id": "6f36b1eb079c18fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:25:37.704690Z",
     "start_time": "2025-08-14T18:25:22.632047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3) Load the E5 model (multilingual), prefer Apple MPS if available ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "model = SentenceTransformer(EMBED_MODEL_ID, device=DEVICE)\n",
    "\n",
    "# ✅ quick probe\n",
    "_probe = model.encode([\"passage: test\", \"passage: deneme\"], normalize_embeddings=True)\n",
    "print(\"Probe embeddings OK, dim:\", _probe.shape[1])"
   ],
   "id": "c011481957ee4b73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Probe embeddings OK, dim: 768\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embed all chunks (with E5 prefixes)\n",
    "\n",
    "Purpose: produce L2-normalized vectors."
   ],
   "id": "e46f92528f9541ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:26:46.642163Z",
     "start_time": "2025-08-14T18:26:35.458052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4) Embed all chunks with required E5 prefix and L2 normalization ---\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "texts = [\"passage: \" + t for t in meta[\"content\"].tolist()]\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE)):\n",
    "    batch = texts[i:i+BATCH_SIZE]\n",
    "    embs = model.encode(batch, normalize_embeddings=True, show_progress_bar=False)\n",
    "    embeddings.append(embs)\n",
    "\n",
    "emb = np.vstack(embeddings).astype(\"float32\")\n",
    "print(\"Emb shape:\", emb.shape)"
   ],
   "id": "987af8cb5642ea74",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:11<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emb shape: (137, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Shape check\n",
    "\n",
    "Purpose: sanity that we have one vector per chunk."
   ],
   "id": "dcaa4d7337c9d4e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:27:11.208327Z",
     "start_time": "2025-08-14T18:27:11.196426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CHECK: embedding shape aligns with meta rows\n",
    "assert emb.shape[0] == len(meta), \"Mismatch: embeddings count != meta rows\"\n",
    "print(\"Embeddings OK:\", emb.shape, \"rows in meta:\", len(meta))"
   ],
   "id": "e5cafec609109230",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings OK: (137, 768) rows in meta: 137\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Build + save FAISS and metadata\n",
    "\n",
    "Purpose: store index & metadata for retrieval."
   ],
   "id": "66195f2d6bd60de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:27:30.740265Z",
     "start_time": "2025-08-14T18:27:30.631362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 5) Build FAISS index (IP on normalized vectors ≈ cosine) and persist ---\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "\n",
    "FAISS_PATH = INDEX_DIR / \"faiss_e5.index\"\n",
    "META_PATH  = INDEX_DIR / \"meta.parquet\"\n",
    "\n",
    "faiss.write_index(index, str(FAISS_PATH))\n",
    "meta.to_parquet(META_PATH, index=False)\n",
    "\n",
    "print(\"Saved index →\", FAISS_PATH)\n",
    "print(\"Saved metadata →\", META_PATH)"
   ],
   "id": "75601d5f71939e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index → /Users/macbook/T3/rag-tekno/data/index/faiss_e5.index\n",
      "Saved metadata → /Users/macbook/T3/rag-tekno/data/index/meta.parquet\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reload + toy query\n",
    "\n",
    "Purpose: verify search round-trip and metadata alignment."
   ],
   "id": "b559a3a8eb9d9c7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:28:01.655644Z",
     "start_time": "2025-08-14T18:27:57.193609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CHECK: reload index, run a toy query using E5 'query:' prefix\n",
    "index2 = faiss.read_index(str(FAISS_PATH))\n",
    "sample_q = meta.iloc[0][\"content\"].split(\"\\n\")[0]  # first question line\n",
    "qvec = model.encode([\"query: \" + sample_q], normalize_embeddings=True).astype(\"float32\")\n",
    "D, I = index2.search(qvec, k=5)\n",
    "\n",
    "print(\"Top-5 distances:\", D[0])\n",
    "print(\"Top-5 rids:\", I[0])\n",
    "print(meta.iloc[I[0]][[\"qa_id\",\"chunk_type\",\"competition\",\"topic\"]])"
   ],
   "id": "3bd3b9a6f74f8492",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 distances: [0.88475597 0.85862666 0.8555093  0.83911455 0.8373387 ]\n",
      "Top-5 rids: [ 0 68 99 27 90]\n",
      "          qa_id chunk_type competition        topic\n",
      "0   ADRES-QG001         qa       ADRES  eligibility\n",
      "68     HSS-Q001         qa         HSS  eligibility\n",
      "99     HSS-Q023         qa         HSS         team\n",
      "27  ADRES-QG024         qa       ADRES  eligibility\n",
      "90     HSS-Q018         qa         HSS         team\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:41:51.775478Z",
     "start_time": "2025-08-14T18:41:51.760645Z"
    }
   },
   "cell_type": "code",
   "source": "assert index2.ntotal == len(meta), \"FAISS index size mismatch\"",
   "id": "20e309029b3779f5",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "69d35a4201a4cea5"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
